# -*- coding: utf-8 -*-
"""Project_R-CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nkG4O1PxjPgdc2hk3TcxkbwO8wKk_iQG

##Project - Image prossecing - Neural network
Simha Franko 209166776

Efrat Anconina 322796749
"""

import os
import zipfile as ZipFile
import shutil
from tensorflow.keras import layers
from tensorflow import keras
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
from keras.applications.vgg16 import VGG16
from keras.layers import Flatten
from keras.layers import Dense
from keras.models import Model
from keras.optimizers import SGD

"""Importing the data"""

!wget --no-check-certificate \
           https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/4drtyfjtfy-1.zip \
           -O /tmp/weather.zip

# extracting all the images
with ZipFile.ZipFile("/tmp/weather.zip", 'r') as zip:
    # printing all the contents of the zip file
    #zip.printdir()
      
    # extracting all the files
    print('Extracting all the files now...')
    zip.extractall()
    print('Done!')
    zip.close()
    print(os.listdir())

with ZipFile.ZipFile("dataset2.zip", 'r') as zip:
    # printing all the contents of the zip file
    #zip.printdir()
  
    # extracting all the files
    print('Extracting all the files now...')
    zip.extractall()
    print('Done!')
    zip.close()
    #print(os.listdir())
    #print(os.listdir("dataset2"))
print(os.listdir("dataset2"))
print(len(os.listdir("dataset2")))

base_dir = 'dataset2'

train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')
# Directory with our training Cloudy
train_Cloudy_dir = "train/Cloudy"
os.makedirs(train_Cloudy_dir)

# Directory with our training Rain 
train_Rain_dir = "train/Rain"
os.makedirs(train_Rain_dir)

# Directory with our training Shine 
train_Shine_dir = "train/Shine"
os.makedirs(train_Shine_dir)

# Directory with our training Sunrise 
train_Sunrise_dir = "train/Sunrise"
os.makedirs(train_Sunrise_dir)


# Directory with our validation Cloudy
validation_Cloudy_dir = "validation/Cloudy"
os.makedirs(validation_Cloudy_dir)

# Directory with our validation Rain 
validation_Rain_dir = "validation/Rain"
os.makedirs(validation_Rain_dir)

# Directory with our validation Shine 
validation_Shine_dir = "validation/Shine"
os.makedirs(validation_Shine_dir)

# Directory with our validation Sunrise 
validation_Sunrise_dir = "validation/Sunrise"
os.makedirs(validation_Sunrise_dir)

print(os.listdir())
print(len(os.listdir()))

"""Adding the dir to all the imges"""

all_pic = []
for img in os.listdir(base_dir):
  all_pic+=["dataset2/" + str(img)]

"""Sorting images by clusters into vectors"""

cloudy_vec = []
rain_vec = []
shine_vec = []
sunrise_vec = []
#for img in os.listdir(base_dir):
for img in all_pic:
  if "cloudy" in img and img not in cloudy_vec:
    cloudy_vec+=[img]
  if "rain" in img and img not in rain_vec:
    rain_vec+=[img]
  if "shine" in img and img not in shine_vec:
    shine_vec+=[img]
  if "sunrise" in img and img not in sunrise_vec:
    sunrise_vec+=[img] 
print(len(cloudy_vec))
print(len(rain_vec)) 
print(len(shine_vec)) 
print(len(sunrise_vec))  
cloudy_vec =list(set(cloudy_vec)) 
print(len(cloudy_vec))

print(cloudy_vec[1])

"""defineing a function that transfering all the dirs to the new dir"""

def transferAllFile(allFile, dest_dir):
  for dir in allFile:
    shutil.move(dir, dest_dir)

"""Let's give the train have 2/3 of the data, and 1/3 to the test"""

#os.chdir(base_dir)
os.listdir()

train_len_cloudy = int(len(cloudy_vec)*(2/3))
transferAllFile(cloudy_vec[:train_len_cloudy], train_Cloudy_dir)
transferAllFile(cloudy_vec[train_len_cloudy:], validation_Cloudy_dir)

train_len_rain = int(len(rain_vec)*(2/3))
transferAllFile(rain_vec[:train_len_rain], train_Rain_dir)
transferAllFile(rain_vec[train_len_rain:], validation_Rain_dir)

train_len_shine = int(len(shine_vec)*(2/3))
transferAllFile(shine_vec[:train_len_shine], train_Shine_dir)
transferAllFile(shine_vec[train_len_shine:], validation_Shine_dir)

train_len_sunrise = int(len(sunrise_vec)*(2/3))
transferAllFile(sunrise_vec[:train_len_sunrise], train_Sunrise_dir)
transferAllFile(sunrise_vec[train_len_sunrise:], validation_Sunrise_dir)

"""##Starting the real thing
### building the CNN with dropout
"""

# Our input feature map is 150x150x3: 150x150 for the image pixels, and 3 for
# the three color channels: R, G, and B

train_dir = "train"
validation_dir = "validation"

model_With_dropout = keras.Sequential(
    [
        layers.Input(shape=(150, 150, 3)),
        # First convolution extracts 16 filters that are 3x3
        # Convolution is followed by max-pooling layer with a 2x2 window
        layers.Conv2D(16, 3, activation='relu'),
        layers.MaxPooling2D(2),
        # Second convolution extracts 32 filters that are 3x3
        # Convolution is followed by max-pooling layer with a 2x2 window
        layers.Conv2D(32, 3, activation='relu'),
        layers.MaxPooling2D(2),
        # Third convolution extracts 64 filters that are 3x3
        # Convolution is followed by max-pooling layer with a 2x2 window
        layers.Conv2D(64, 3, activation='relu'),
        layers.MaxPooling2D(2),
        # Flatten feature map to a 1-dim tensor so we can add fully connected layers
        layers.Flatten(),
        layers.Dropout(0.5), #only change!
        layers.Dense(512, activation='relu'),
        layers.Dense(4, activation='sigmoid'),
    ]
)

#model.summary()

model_With_dropout.compile(loss='binary_crossentropy',
              optimizer=RMSprop(lr=0.001),
              metrics=['acc'])

# All images will be rescaled by 1./255
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)

# Flow training images in batches of 20 using train_datagen generator
train_generator = train_datagen.flow_from_directory(
        train_dir,  # This is the source directory for training images
        target_size=(150, 150),  # All images will be resized to 150x150
        batch_size=7,
        # Since we use binary_crossentropy loss, we need binary labels
        class_mode='categorical')

# Flow validation images in batches of 20 using val_datagen generator
validation_generator = val_datagen.flow_from_directory(
        validation_dir,
        target_size=(150, 150),
        batch_size=4,
        class_mode='categorical')

history = model_With_dropout.fit_generator(
      train_generator,
      steps_per_epoch=107,  # 749 images = batch_size * steps
      epochs=10,
      validation_data=validation_generator,
      validation_steps=94,  # 376 images = batch_size * steps
      verbose=2)

"""Ploting the results"""

# Retrieve a list of accuracy results on training and validation data
# sets for each training epoch
acc = history.history['acc']
print("Accuracy of traning: " + str(acc[-1]))
val_acc = history.history['val_acc']
print("Accuracy of testing: " + str(val_acc[-1]))
print("The diffrance between Accuracies: " +str(acc[-1]-val_acc[-1]))

# Retrieve a list of list results on training and validation data
# sets for each training epoch
loss = history.history['loss']
val_loss = history.history['val_loss']

# Get number of epochs
epochs = range(len(acc))

# Plot training and validation accuracy per epoch
plt.plot(epochs, acc)
plt.plot(epochs, val_acc)
plt.title('Training and validation accuracy')

plt.figure()

# Plot training and validation loss per epoch
plt.plot(epochs, loss)
plt.plot(epochs, val_loss)
plt.title('Training and validation loss')

"""### Building the CNN with dropout and augmentation"""

# Adding rescale, rotation_range, width_shift_range, height_shift_range,
# shear_range, zoom_range, and horizontal flip to our ImageDataGenerator
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,)

# Note that the validation data should not be augmented!
val_datagen = ImageDataGenerator(rescale=1./255)

# Flow training images in batches of 7 using train_datagen generator
train_generator = train_datagen.flow_from_directory(
        train_dir,  # This is the source directory for training images
        target_size=(150, 150),  # All images will be resized to 150x150
        batch_size=20,
        # Since we use binary_crossentropy loss, we need binary labels
        class_mode='categorical')

# Flow validation images in batches of 4 using val_datagen generator
validation_generator = val_datagen.flow_from_directory(
        validation_dir,
        target_size=(150, 150),
        batch_size=20,
        class_mode='categorical')

model_with_augmentation = keras.Sequential(
    [
        layers.Input(shape=(150, 150, 3)),
        # First convolution extracts 16 filters that are 3x3
        # Convolution is followed by max-pooling layer with a 2x2 window
        layers.Conv2D(16, 3, activation='relu'),
        layers.MaxPooling2D(2),
        # Second convolution extracts 32 filters that are 3x3
        # Convolution is followed by max-pooling layer with a 2x2 window
        layers.Conv2D(32, 3, activation='relu'),
        layers.MaxPooling2D(2),
        # Third convolution extracts 64 filters that are 3x3
        # Convolution is followed by max-pooling layer with a 2x2 window
        layers.Conv2D(64, 3, activation='relu'),
        layers.MaxPooling2D(2),
        # Flatten feature map to a 1-dim tensor so we can add fully connected layers
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(512, activation='relu'),
        layers.Dense(4, activation='sigmoid'),
    ]
)

#model_with_augmentation.summary()

model_with_augmentation.compile(loss='binary_crossentropy',
              optimizer=RMSprop(lr=0.001),
              metrics=['acc'])

history = model_with_augmentation.fit_generator(
      train_generator,
      steps_per_epoch=37,
      epochs=20,
      validation_data=validation_generator,
      validation_steps=18,
      verbose=2)

# Retrieve a list of accuracy results on training and validation data
# sets for each training epoch
acc = history.history['acc']
print("Accuracy of traning: " + str(acc[-1]))
val_acc = history.history['val_acc']
print("Accuracy of testing: " + str(val_acc[-1]))
print("The diffrance between Accuracies: " +str(acc[-1]-val_acc[-1]))

# Retrieve a list of list results on training and validation data
# sets for each training epoch
loss = history.history['loss']
val_loss = history.history['val_loss']

# Get number of epochs
epochs = range(len(acc))

# Plot training and validation accuracy per epoch
plt.plot(epochs, acc)
plt.plot(epochs, val_acc)
plt.title('Training and validation accuracy')

plt.figure()

# Plot training and validation loss per epoch
plt.plot(epochs, loss)
plt.plot(epochs, val_loss)
plt.title('Training and validation loss')

"""### Building the CNN with Transfer Learning"""

# vgg16 model used for transfer learning on the dogs and cats dataset

# define cnn model
def define_model():
	# load model
	model = VGG16(include_top=False, input_shape=(224, 224, 3))
	# mark loaded layers as not trainable
	for layer in model.layers:
		layer.trainable = False
	# add new classifier layers
	flat1 = Flatten()(model.layers[-1].output)
	class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)
	output = Dense(4, activation='sigmoid')(class1)
	# define new model
	model = Model(inputs=model.inputs, outputs=output)
	# compile model
	opt = SGD(lr=0.001, momentum=0.9)
	model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])
	return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
	# plot loss
	# Retrieve a list of accuracy results on training and validation data
  # sets for each training epoch
  acc = history.history['acc']
  print("Accuracy of traning: " + str(acc[-1]))
  val_acc = history.history['val_acc']
  print("Accuracy of testing: " + str(val_acc[-1]))
  print("The diffrance between Accuracies: " +str(acc[-1]-val_acc[-1]))

  # Retrieve a list of list results on training and validation data
  # sets for each training epoch
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  # Get number of epochs
  epochs = range(len(acc))

  # Plot training and validation accuracy per epoch
  plt.plot(epochs, acc)
  plt.plot(epochs, val_acc)
  plt.title('Training and validation accuracy')

  plt.figure()

  # Plot training and validation loss per epoch
  plt.plot(epochs, loss)
  plt.plot(epochs, val_loss)
  plt.title('Training and validation loss')
 
# run the test harness for evaluating a model
def run_test_harness():
	# define model
	model = define_model()
	# create data generator
	datagen = ImageDataGenerator(featurewise_center=True)
	# specify imagenet mean values for centering
	datagen.mean = [123.68, 116.779, 103.939]
	# prepare iterator
	train_it = datagen.flow_from_directory(train_dir,
		class_mode='categorical', batch_size=64, target_size=(224, 224))
	test_it = datagen.flow_from_directory(validation_dir,
		class_mode='categorical', batch_size=64, target_size=(224, 224))
	# fit model
	history = model.fit_generator(train_it, steps_per_epoch=len(train_it),
		validation_data=test_it, validation_steps=len(test_it), epochs=10, verbose=1)
	# evaluate model
	_, acc = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)
	print('> %.3f' % (acc * 100.0))
	# learning curves
	summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()